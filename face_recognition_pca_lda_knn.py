# -*- coding: utf-8 -*-
"""Rawan mohamed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pAIwuqWhtTuQgMzdSFgDpkniXsz9PuYT

rawan mohamed farouk


rawan elwardany


shadwa
"""

import numpy as np
import pandas as pd
import cv2 # for reading pics
import matplotlib.pyplot as plt
import os #  used to list the contents of a directory
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

"""Understand the Format"""

# The dataset has 10 images per 40 subjects. Every image is a grayscale image of size 92x112.
arr = cv2.imread('./images/s1/1.pgm', 0)
print(len(arr[0]))
print(len(arr))
print(arr) # array of pixles

"""Display the first image"""

# Read the first face image
arr = cv2.imread('./images/s1/1.pgm')
#display the first image.
plt.imshow(arr, cmap='gray')
plt.title('First Image')
plt.axis('off')
plt.show()

"""Generate the Data Matrix and the Label vector"""

data=[]
stack=[]

#Convert every image into a vector of 10304 values corresponding to the image size.

for folder in range(1,41):
	# os.listdir ==> take file(directory) path as input and return entries inside it.
	images = os.listdir('./images/s'+str(folder))
	for image in images:
		img = cv2.imread('./images/s'+str(folder)+"/"+image,0)
		height1, width1 = img.shape[:2]
		img_col = np.array(img, dtype='float64').flatten()
		data.append(img_col )

		#Stack the 400 vectors into a single Data Matrix D and generate the label vector y.
		stack.append(int(folder))

# image to a vector of 10304
print(len(data[0]))

"""Split the Dataset into Training and Test sets"""

df = pd.DataFrame(data=data)
df.head()

# The labels are integers from 1:40 corresponding to the subject id
df["label"] = stack
print(df)

"""Split data"""

testData = df[df.index % 2 == 0]  #even rows for testing
trainData = df[df.index % 2 != 0]   #odd rows for training

"""Separate data from labels"""

trainFaces=trainData.drop(["label"],axis=1)
testFaces=testData.drop(["label"],axis=1)
trainTestLabels = trainData['label']

"""PCA"""

# Use Dtrain only as this PCA is an unsupervised method
def pca_fun(Dtrain , alpha): # alpha ==> size of variance after projection

    #center the data (by subrtracting from the mean) , np.mean(Dtrain , axis = 0)==>Computes the mean of each column (feature) in the dataset.
    D_meaned = Dtrain - np.mean(Dtrain , axis = 0)

    #compute covariance matrix
    cov_mat = np.cov(D_meaned , rowvar = False)

    #compute eigenvalues & eigenvectors
    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)

    # sort the eigenvalues and eigenvectors in descending order.
    sorted_index = np.argsort(eigen_values)[::-1]
    sorted_eigenvalue = eigen_values[sorted_index]
    sorted_eigenvectors = eigen_vectors[:,sorted_index]

    #choose dimensionality
    # to determine the number of principal components(directions) required to retain the specified alpha (variance).
    # The first principal component captures the mostvariance, the second captures the second most, and so on.
    count=0
    var_exp=[]
    sumEigenvalues = sum(sorted_eigenvalue) #sum of eigenvalues.
    #loop over the sorted eigenvalues , until the cumulative variance (count) equal or more alpha.
    for i in sorted_eigenvalue :
      if count<alpha :
        v = i / sumEigenvalues # current
        var_exp.append(v)
        count+=v # Updates the cumulative variance
      else:
        break
    num_components = len(var_exp) #Determines the number of principal components needed to achieve the desired variance.
    print("num_components = ",num_components)

    #reduced basis
    #to select the subset of eigenvectors corresponding to the top num_components eigenvalues.
    eigenvector_subset = sorted_eigenvectors[:,0:num_components]

    #reduced dimensionality data
    #Performs matrix dot multiplication to transform the original data into the reduced-dimensional space defined by eigenvector_subset.
    D_reduced_train = np.dot(D_meaned, eigenvector_subset)

    return D_reduced_train,eigenvector_subset

"""Applying pca (my function) on the train dataset(without labels)"""

reducedTrainData,EVectors=pca_fun(trainFaces,0.8)
print(reducedTrainData)

"""For checking the answers of my function , use PCA function from sklearn library"""

# PCA from sklearn library
from sklearn.decomposition import PCA
pcaBuiltIn = PCA(n_components=36)
X = pcaBuiltIn.fit_transform(trainFaces)
print(X)

"""Show the first 5 eigen faces"""

for i in range(5):
    plt.imshow(EVectors[:,i].reshape(112,92),cmap='gray')
    plt.show()

"""KNN with PCA"""

# KNN function with k = 1
def simpleClassifier(reducedTrainData,TestData,trainTestLabels):
  x = KNeighborsClassifier(n_neighbors=1, weights='distance')
  x.fit(reducedTrainData, trainTestLabels)
  score = x.score(TestData, trainTestLabels)
  y_pred=x.predict(TestData)
  print(classification_report(trainTestLabels, y_pred))
  return score

alphas=[0.8,0.85,0.9,0.95]
accuracies = [0,0,0,0]
counter = 0
test_data = testFaces
for alpha in alphas:
  train_data, eigen_vector= pca_fun(trainFaces,alphas[counter])
  D_meaned_test = testFaces - np.mean(testFaces , axis = 0)
  test_data = np.dot(D_meaned_test, eigen_vector)
  accuracies[counter] = simpleClassifier(train_data,test_data,trainTestLabels)
  counter+=1

plt.scatter(alphas, accuracies)
plt.show()

# Support: The count of true instances for each class in the dataset.

"""Classifier Tuning"""

# k = 1,3,5,7
alphas=[0.8,0.85,0.9,0.95]
for alpha in alphas:
  Knn= [1,3,5,7]
  accuracies = [0,0,0,0]
  train_data, eigen_vector= pca(trainFaces,alpha)
  D_meaned_test = testFaces - np.mean(testFaces , axis = 0)
  test_data = np.dot(D_meaned_test, eigen_vector)
  counter = 0
  for K in Knn:
    neigh = KNeighborsClassifier(n_neighbors=K, weights='distance')
    neigh.fit(train_data, trainTestLabels)
    score = neigh.score(test_data, trainTestLabels)
    y_pred=neigh.predict(test_data)
    print(classification_report(trainTestLabels, y_pred))
    accuracies[counter] = score
    counter += 1
  print("Alpha : " , alpha)
  plt.scatter(Knn, accuracies)
  plt.show()

"""LDA

1) get mean matrix (mean of all classes)
"""

# function to get mean
# 10305 ==> 10304 for data + 1 for label
def mean_matrix(train_Data, classesNumber):
    num_features = train_Data.shape[1]  # including label column
    means = np.zeros((classesNumber, num_features))

    for i in range(1, classesNumber + 1):
        temp = np.zeros((1, num_features))
        D = train_Data[train_Data['label'] == i]
        for j in D.index:
            temp += train_Data.loc[j].values.reshape(1, -1)

        if len(D) > 0:  # Avoid division by zero
            temp /= len(D)
        temp[0][-1] = i  # Assign class label to the last element of the row
        means[i - 1] = temp

    return means

# pass the dataframe(which contains data and labels) to the function
meanMatrix = mean_matrix(df, 40)

print(meanMatrix)
print(meanMatrix.shape)

totalMean = np.mean(trainData, axis=0)
totalMean = totalMean[:-1]

print(totalMean)

"""2) get between-class scatter matrix"""

def between_class_scatter_matrix(trainData,meanMatrix,totalMean,classesNumber):
  Sb = np.zeros((10304,10304))
  for m in range (0,classesNumber) :
    temp = meanMatrix[m] - totalMean

    #1*10304
    temp = np.array([temp])

    D = trainData[trainData.label == m+1]
    nk = len(D)

    Sb += nk * np.dot(np.transpose(temp),temp)

  return Sb

Sb = between_class_scatter_matrix(trainData,meanMatrix[:,:-1] ,totalMean,40)
print(Sb)
print(Sb.shape)

def center_class_matrices(trainData,meanMatrix,classesNumber) :
  Z = np.zeros(trainData.shape)
  index = 0
  for i in range (1,classesNumber+1) :

    D = trainData[trainData.label == i]
    for j in D.index :
      temp = trainData[trainData.index == j] - meanMatrix[i-1]
      Z[index] = np.array(temp)[0]
      index += 1


  return Z[:,:-1]

Z = center_class_matrices(trainData,meanMatrix,40)
print(Z)
print (Z.shape)

def class_scatter_matrix_one(trainData,Z,classesNumber):
  S = np.zeros((10304, 10304))
  for i in range (0,classesNumber) :

    D = trainData[trainData.label == i+1]
    nk = len(D)

    index = i*nk
    temp = Z[index:index+nk,:]
    S += np.dot(np.transpose(temp),temp)
  return S

S = class_scatter_matrix_one(trainData,Z,40)
print(S)
print(S.shape)

from scipy.sparse.linalg import eigs
W = np.dot(np.linalg.inv(S),Sb)
eigen_values , eigen_vectors = np.linalg.eigh(W)

sorted_index = np.argsort(eigen_values)[::-1]
sorted_eigenvalue = eigen_values[sorted_index]
sorted_eigenvectors = eigen_vectors[:,sorted_index]

print(sorted_eigenvectors)
print(sorted_eigenvectors.shape)

U = sorted_eigenvectors[:,0:39]
print(U.shape)

def get_reduced_data(trainData,testData,U):

  D_reduced_train = np.dot(trainData,U)
  D_reduced_test = np.dot(testData,U)

  return D_reduced_train,D_reduced_test


U = sorted_eigenvectors[:,0:39]
trainFaces=trainData.drop(["label"],axis=1)
testFaces=testData.drop(["label"],axis=1)

D_reduced_train , D_reduced_test = get_reduced_data(trainFaces,testFaces,U)

print(D_reduced_train)
print(D_reduced_train.shape)
print(D_reduced_test)
print(D_reduced_test.shape)

"""Show the first 5 eigen faces"""

for i in range(5):
    plt.imshow(sorted_eigenvectors[:,i].reshape(112,92),cmap='gray')
    plt.show()

"""KNN with LDA"""

# KNN with k = 1
def simpleClassifier(reducedTrainData,reducedTestData,trainTestLabels):
  neigh = KNeighborsClassifier(n_neighbors=1, weights='distance')
  neigh.fit(reducedTrainData, trainTestLabels)
  score = neigh.score(reducedTestData, trainTestLabels)
  y_pred=neigh.predict(reducedTestData)
  print(classification_report(trainTestLabels, y_pred))
  return score

simpleClassifier(D_reduced_train,D_reduced_test,trainData['label'])

"""Comparing with PCA
using k = 1 gives 0.95 accuracy in LDA , while it give 0.94 in PCA

Classifier Tuning
"""

Knn=[1,3,5,7]

def score_calc_LDA(Knn,projected_matrix_training, projected_matrix_testing, split_labels_vectors):
    accuracy = [0,0,0,0]
    y_pred = []
    for i in range(len(Knn)):
        x = KNeighborsClassifier(n_neighbors=Knn[i], weights='distance')
        x.fit(projected_matrix_training, split_labels_vectors)
        accuracy[i] = x.score(projected_matrix_testing, split_labels_vectors)
        y_pred.append(x.predict(projected_matrix_testing))
    plt.scatter(Knn, accuracy)
    plt.show()
    print(classification_report(split_labels_vectors, y_pred[0]))
    print(accuracy)

score_calc_LDA(Knn,D_reduced_train,D_reduced_test,trainData['label'])

def LDAalgorithm(trainData,testData,classesNumber) :

  meanMatrix = mean_matrix(trainData,classesNumber)

  totalMean = np.mean(trainData, axis=0)
  totalMean = totalMean[:-1]

  Sb = between_class_scatter_matrix(trainData,meanMatrix[:,:-1] ,totalMean,classesNumber)

  Z = center_class_matrices(trainData,meanMatrix,classesNumber)

  S = class_scatter_matrix_one(trainData,Z,classesNumber)

  W = np.dot(np.linalg.inv(S),Sb)
  eigen_values , eigen_vectors = np.linalg.eigh(W)

  sorted_index = np.argsort(eigen_values)[::-1]
  sorted_eigenvalue = eigen_values[sorted_index]
  sorted_eigenvectors = eigen_vectors[:,sorted_index]

  U = sorted_eigenvectors[:,0:classesNumber-1]


  trainFaces = trainData.drop(["label"],axis=1)
  testFaces = testData.drop(["label"],axis=1)
  Data_reduced_train , Data_reduced_test = get_reduced_data(trainFaces,testFaces,U)

  return Data_reduced_train , Data_reduced_test

"""Bonus

B) Change the number of instances per subject to be 7 and keep 3 instances per subject for testing.
"""

# 70% for training , 30% for testing.
from sklearn.model_selection import train_test_split
x = df.drop(['label'],axis=1)
y = df['label']
x_train, x_test, y_train, y_test = train_test_split(
      x, y, test_size=0.3, random_state=100,stratify=y)

print(x_train)
print(y_train)

"""KNN with PCA after the new splitting."""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

alphas=[0.8,0.85,0.9,0.95]

for alpha in alphas:
  Knn=[1,3,5,7]
  accuracies = [0,0,0,0]
  counter = 0
  newReducedTraindata,newEVectors=pca_fun(x_train,alpha) # applying pca
  D_meaned_test = x_test - np.mean(x_test , axis = 0)
  newReducedTestdata = np.dot(D_meaned_test, newEVectors)
  for k in Knn:
    neigh = KNeighborsClassifier(n_neighbors=k, weights='distance')
    neigh.fit(newReducedTraindata, y_train)
    score = neigh.score(newReducedTestdata, y_test)
    y_pred=neigh.predict(newReducedTestdata)
    print(classification_report(y_test, y_pred))
    accuracies[counter] = score
    counter = counter+1

  print("Alpha : " , alpha)
  plt.scatter(Knn, accuracies)
  plt.show()

"""Applying LDA after the new splitting"""

dfTrain = pd.DataFrame(data=x_train)
dfTrain['label']=y_train

dfTest = pd.DataFrame(data=x_test)
dfTest['label']=y_test

Data_reduced_train , Data_reduced_test = LDAalgorithm(dfTrain,dfTest,40)

print(Data_reduced_train)
print(Data_reduced_train.shape)
print(Data_reduced_test)
print(Data_reduced_test.shape)

Knn=[1,3,5,7]
accuracylda = [0,0,0,0]
y_pred = []

for i in range(len(Knn)):
  n = KNeighborsClassifier(n_neighbors=Knn[i], weights='distance')
  n.fit(Data_reduced_train,y_train)
  accuracylda[i] =  n.score(Data_reduced_test, y_test)
  y_pred.append(n.predict(Data_reduced_test))
plt.scatter(Knn, accuracylda)
plt.show()

print(classification_report(y_test, y_pred[0]))
print(accuracylda)
